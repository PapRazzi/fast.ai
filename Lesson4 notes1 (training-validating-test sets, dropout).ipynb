{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Validation/Test\n",
    "When creating our model, note that we have 3 sets:\n",
    "\n",
    "* The training set\n",
    "* The validation set\n",
    "* The test set\n",
    "\n",
    "The training set is used with stochastic gradient descent to train the various weights within the network.\n",
    "\n",
    "The test set is used to check how good our model is -- can it correctly identify data it hasn't seen before?\n",
    "\n",
    "But what about the validation set?\n",
    "\n",
    "Fast.ai tells us the validation set is used to determine what kind of model to use, but let's dig deeper.\n",
    "\n",
    "We must go back to the concept of _underfitting_ and _overfitting_.\n",
    "\n",
    "Thinking of it like linear regression, if a model is _underfitting_, it would be like there was a straight line going through the data points, but the line is a bit to general to accurately predict future data points.\n",
    "\n",
    "If a model is _overfitting_, it is like the line hits all of the data points exactly. This might seem fine, but the line would be so random that it wouldn't be able to predict any new, unseen data.\n",
    "\n",
    "(Reminder: You can check if your model is overfitting because the training loss will be a lot lower than the validation loss.)\n",
    "\n",
    "What you want is a line that is _just right_. That is, it goes through the data points pretty closely, but there is still a pattern to the line, so you can use it to predict future data.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/pekoto/fast.ai/master/images/fitting.jpg\" height=300 width=500>\n",
    "\n",
    "The problem with just using training sets and test sets is that we might end up overfitting to to the test set as well.\n",
    "\n",
    "What we want to do is test on data we've never seen before.\n",
    "\n",
    "So we take a random 20% of the training set, and make that our validation set. That way we ensure we test against a random 20% each time. Also, we might not have the answers for the test set -- the whole point is that it's unseen data. But since the validation set comes from the training set, we know what it's meant to be.\n",
    "\n",
    "Reference:\n",
    "http://www.fast.ai/2017/11/13/validation-sets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "When we are using a pretrained convolutional neural network (CNN), and we add a new layer to it, we are actually adding several new layers. After declaring your pretrained net as `learn = ...`, you can type `learn` to see which layers it added. By default tey will look like this:\n",
    "\n",
    "* BatchNorm1d: Covered later\n",
    "* Dropout\n",
    "* Linear: Does matrix multiplication -- takes the output from a convolution layer as input, and outputs to the number of classes\n",
    "* ReLU: Gets rid of -ves\n",
    "* BatchNorm1d\n",
    "* Dropout\n",
    "* Linear\n",
    "* LogSoftmax: The softmax layer to get a prediction (it uses log just for numerical accuracy sake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b0ccb6f86b4e6db880d1f404acdf46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PATH=\"data/dogbreed/\"\n",
    "size=224\n",
    "architecture=resnext101_64\n",
    "batch_size=58\n",
    "\n",
    "label_csv = f'{PATH}labels.csv'\n",
    "\n",
    "# num of rows -1 (to account for the header)\n",
    "n = len(list(open(label_csv)))-1\n",
    "\n",
    "# random 20% of rows to use as the validation set\n",
    "# get cross validation indexes\n",
    "val_idxs = get_cv_idxs(n)\n",
    "\n",
    "# We're going to use data augmentation, so let's set up our transforms\n",
    "transforms = tfms_from_model(architecture, size, aug_tfms=transforms_side_on, max_zoom=1.1)\n",
    "\n",
    "# Convenience method\n",
    "def get_data(size, batch_size):\n",
    "    transforms = tfms_from_model(architecture, size, aug_tfms=transforms_side_on, max_zoom=1.1)\n",
    "    data = ImageClassifierData.from_csv(PATH, 'train', f'{PATH}labels.csv', test_name='test', num_workers=4,\n",
    "                                       val_idxs=val_idxs, suffix='.jpg', tfms=transforms, bs=batch_size)\n",
    "    return data if size>300 else data.resize(340, 'tmp')\n",
    "\n",
    "data = get_data(size, batch_size)\n",
    "\n",
    "learn = ConvLearner.pretrained(architecture, data, precompute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (1): Dropout(p=0.25)\n",
       "  (2): Linear(in_features=4096, out_features=512)\n",
       "  (3): ReLU()\n",
       "  (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (5): Dropout(p=0.5)\n",
       "  (6): Linear(in_features=512, out_features=120)\n",
       "  (7): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the __dropout__ layer doing?\n",
    "\n",
    "Essentially, if we set p (probability) = 0.5, it just goes through all of the activations in our filtered image and for each activation, there is a 50% change we just drop it. I.e., we get rid of 50% of the activation.\n",
    "\n",
    "We keep roughly the same shape, and the other activations are doubled to to keep the average activation the same. So why perform dropout?\n",
    "\n",
    "Simply, it stops the data from overfitting. By randomly dropping a certain number of activations, we ensure our data is always a little different, and so we stop overfitting. It's making the activations more random, solves the generalization problem.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/pekoto/fast.ai/master/images/dropout.jpg\" width=400 height=300>\n",
    "\n",
    "The default dropout probabilities (ps) should work well, but if it's overfitting you can try bumping them up, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can set the probability dropout for different layers like this\n",
    "learn = ConvLearner.pretrained(architecture, data, ps=[0.], precompute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
