{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs)\n",
    "Convolutional neural networks are the workhorses of image recognition.\n",
    "\n",
    "The filters (a.k.a, image kernels) and the weights in fully connected layers are learned using stochastic gradient descent (minimizing the cost function -- the measure of difference between expected and actual), but in this explanation, we will assume we are using a pretrained network where these values are already learned.\n",
    "\n",
    "## Features\n",
    "First principle: an image can be made up of a number of features. For example, an X could be thought of as being made up of upper_left-to-lower_right diagonals, upper_right-to-lower_left diagonals, and a cross in the middle.\n",
    "\n",
    "<img src=\"images/cnn1.jpg\" height=400, width=500>\n",
    "\n",
    "These filters to detect certain features can be called __filters__ or __kernels__.\n",
    "\n",
    "Now, every pixel in our image has some value -- for example, a grayscale value (or, in the case of a coloured image, 3 values -- R, G, and B.)\n",
    "\n",
    "<img src=\"images/cnn2.jpg\" height=300, width=300>\n",
    "\n",
    "## The layers in a CNN\n",
    "In a CNN, we essentially have 4 types of layer:\n",
    "\n",
    "1. Convolution layer (detect features)\n",
    "2. ReLU layer (turn -ves into 0s to stop math blowing up)\n",
    "3. Max pooling layer (shrink the image resolution to save memory/increase speed)\n",
    "4. Fully connected layer (multiply pixel values by weights to guess what the object is)\n",
    "\n",
    "Exactly how you organize these layers and how many of them you have -- i.e., organizing the hyperparameters -- is kind of an art.\n",
    "\n",
    "### 1. Convolution layer\n",
    "In the convolution layer, we take our filters and move them across every pixel in the image.\n",
    "At each position, we:\n",
    "\n",
    "1. Multiply each kernel value by each corresponding pixel value\n",
    "2. Add up all of these multiplied values\n",
    "3. Divide by the number of pixels (at least some CNNs seem to do this, others don't)\n",
    "\n",
    "This will leave us with a stack of filtered images, where we have a value for each pixel. We should be able to see the patterns in these matched images. In the filtered images, high activations will map to where the feature appeared. So already we've started to detect parts of the images, such as edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1],\n",
       "       [ 0,  0,  0],\n",
       "       [-1, -1, -1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For example to detect horizontal lines, we might use a filter (kernel) like this:\n",
    "import numpy as np\n",
    "\n",
    "horizontal_line_kernel = np.array([[1, 1, 1],\n",
    "                                   [0, 0, 0,],\n",
    "                                   [-1, -1, -1]])\n",
    "\n",
    "horizontal_line_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think about it, applying this kernel will give high activations where we have a horizontal line with blank space underneath. If we have a vertical line, the 1s on the top row will be cancelled out by the -1s on the bottom row, so vertical lines will be ignored.\n",
    "\n",
    "We can see an example of this filter being used in conv_example.ods.\n",
    "\n",
    "<img src=\"images/cnn3.jpg\" height=500, width=500>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  0, -1],\n",
       "       [ 1,  0, -1],\n",
       "       [ 1,  0, -1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Likewise we could detect vertical lines by using a kernal like this:\n",
    "\n",
    "vertical_line_kernel = np.array([[1, 0, -1],\n",
    "                                 [1, 0, -1],\n",
    "                                 [1, 0, -1]])\n",
    "\n",
    "vertical_line_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we could detect vertical lines too:\n",
    "\n",
    "<img src=\"images/cnn4.jpg\" height=500, width=500>\n",
    "\n",
    "So in our first layer, a convolution layer, we have picked out the horizontal and vertical lines from our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
