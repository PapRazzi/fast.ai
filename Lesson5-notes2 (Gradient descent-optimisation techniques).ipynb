{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent/Optimization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian\n",
    "The Jacobian matrix stores all of the first order partial derivatives of a vector valued function.\n",
    "(Reminder: vector valued function = a function with multiple output values [into a vector]).\n",
    "\n",
    "E.g.,\n",
    "\n",
    "$f(x,y) = \\begin{equation}\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  x^2y\\\\\n",
    "  5x + sin y\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{equation}$\n",
    "\n",
    "Then we can break it down into:\n",
    "\n",
    "$f_1(x,y) = x^2y$\n",
    "\n",
    "and\n",
    "\n",
    "$f_2(x,y) = 5x + sin y$\n",
    "\n",
    "Then the Jacobian matrix becomes:\n",
    "\n",
    "$J_f(x,y) = \\begin{equation}\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  \\frac{\\delta{f_1}}{\\delta{x}} \\frac{\\delta{f_1}}{\\delta{y}}\\\\\n",
    "  \\frac{\\delta{f_2}}{\\delta{x}} \\frac{\\delta{f_2}}{\\delta{y}}\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{equation}$\n",
    "\n",
    "$J_f(x,y) = \\begin{equation}\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  2xy  &x^2\\\\\n",
    "  5 &cos y\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{equation}$\n",
    "\n",
    "The __Hessian__ matrix is similar, but it stores the second order partial derivatives (the derivative of the derivative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "The gradient descent training process is as follows:\n",
    "\n",
    "1. Randomly choose some weights for your network\n",
    "2. Perform a forward pass on your data to get predictions for your data\n",
    "3. Use a loss function to calculate how well your network is performing. The loss function will calculate some measure of difference between your predictions and the correct values. The lower the loss function, the better.\n",
    "4. Calculate the gradient: this is the derivative of the cost function. Recall that a gradient will give you the slope of steepest ascent.\n",
    "5. Update your weights by doing `weights - (gradient*learning rate)`. This will minimize the loss function, since we're moving \"downhill\", and so improve our weights. (Make the weights closer to values that will give more accurate predictions.)\n",
    "6. Repeat steps 2-5\n",
    "7. Once we have processed all of our data, repeat 2-6 for another epoch, and so on until we have good weights\n",
    "\n",
    "#### \"Stochastic\"\n",
    "\n",
    "What makes gradient descent __stochastic__ is that we evaluate our loss (aka error or cost) using just a subset of a data -- a mini-batch.\n",
    "\n",
    "#### Backpropagation\n",
    "\n",
    "Since our cost function involves calculating a prediction by using the various activation functions in our network (weighted sums, ReLU, Sigmoid, etc.), taking the derivative of this function will necessarily involve taking the derivative of our entire network. In practical terms, this is implemented by treating of each activation as a \"gate\" in a \"circuit\". When we're doing our forward pass, we also calculate the derivative of each \"gate\" and then use this derivative during the backward pass. During the backward pass, we just keep applying the chain rule to work out what effect this gate (mathematical operation) has on the final output.\n",
    "\n",
    "This lecture has a good walkthrough of the process: https://www.youtube.com/watch?v=i94OvYb6noo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
